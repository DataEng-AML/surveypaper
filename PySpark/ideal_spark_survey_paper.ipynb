{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import requests as req\n",
    "import zipfile as zf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sweetviz as sv # to print df profile\n",
    "import glob\n",
    "import gzip\n",
    "import shutil\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, col, avg, min, max, stddev, count,\\\n",
    "                                    countDistinct, to_timestamp, col, when, split, regexp_replace,\\\n",
    "                                    dayofmonth, month, year, hour, minute, second\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 19:59:52 WARN Utils: Your hostname, MD-061377 resolves to a loopback address: 127.0.1.1; using 192.168.1.57 instead (on interface wlp0s20f3)\n",
      "24/03/28 19:59:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/28 19:59:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/28 19:59:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/28 19:59:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/03/28 19:59:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "# Start the Spark session\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"IDEAL_dataset_spark_pipeline\") \\\n",
    "    .config(\"spark.jars\", \"/home/abcxyz/userid123/Downloads/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download metadata zipped file from the url metadata_url\n",
    "metadata_url = \"https://datashare.ed.ac.uk/bitstream/handle/10283/3647/metadata_and_surveys.zip?sequence=20&isAllowed=y\"\n",
    "mtdt_download = req.get(metadata_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download household_sensors (hhs) zipped file containing ,csv.gz files\n",
    "# very large file over 15GB zipped file initially downloaded and \n",
    "# extracted with 128GB RAM Ubuntu machine, sample extract used here.\n",
    "\n",
    "#hhs_url=\"https://datashare.ed.ac.uk/bitstream/handle/10283/3647/household_sensors.zip?sequence=25&isAllowed=y\"\n",
    "#hhsensor_download = req.get(household_sensors_url)\n",
    "\n",
    "# Note: the sample data has been provided in https://github.com/DataEng-AML/surveypaper/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata/weatherfeed\n",
      "metadata/meterreading\n",
      "metadata/location\n",
      "metadata/room\n",
      "metadata/tariff\n",
      "metadata/sensor\n",
      "metadata/home\n",
      "metadata/person\n",
      "metadata/other_appliance\n",
      "metadata/appliance\n",
      "metadata/sensorbox\n"
     ]
    }
   ],
   "source": [
    "# Extract the metadata and survey files but interest only in the metadata files as listed\n",
    "dstpath = '/home/abcxyz/userid123/Downloads'\n",
    "allfiles=[os.path.join(dstpath,file) for file in os.listdir(dstpath) if file.endswith(\".csv\")]#check for csv files\n",
    "with open(\"metadata_and_surveys.zip\", \"wb\") as each_csv: # open the metadata_and_surveys.zip file\n",
    "    each_csv.write(mtdt_download.content) # write the downloaded file into each_csv\n",
    "    with zf.ZipFile(\"metadata_and_surveys.zip\",\"r\") as myzip: # open the zip file\n",
    "        myzip.extractall(\"metadata_and_surveys\") # extract the zip file\n",
    "        for file in myzip.namelist(): # iterate over each extracted file\n",
    "            ext = os.path.splitext(file)[1] # retrieve the file extension\n",
    "            path_name = os.path.splitext(file)[0] # retrieve the basename\n",
    "            if ext == \".csv\" and \"metadata\" in path_name: # check if the file in metadata folder is a csv\n",
    "                print(path_name) # print the basename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign directories to receive the household sensor data\n",
    "\n",
    "# path to sample household sensor data\n",
    "hh_file_path = '/home/abcxyz/userid123/Downloads/household_sensors.zip'\n",
    "\n",
    "# existing path to receive the unzipped output from .zip\n",
    "hh_unzip_loc = '/home/abcxyz/userid123/household_sensors'\n",
    "\n",
    "# existing path for the extracted csv from the .csv.gz output\n",
    "hh_csv_loc = '/home/abcxyz/userid123/household_sensors_csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home308_outside2829_sensor19926_gas-pulse_gas.csv\n",
      "home308_livingroom2828_sensor19920c19924_electric-mains_electric-combined.csv\n",
      "home308_kitchen2831_sensor19973_tempprobe_hot-water-hot-pipe.csv\n",
      "home308_kitchen2831_sensor19972_tempprobe_hot-water-cold-pipe.csv\n",
      "home308_kitchen2831_sensor19967_tempprobe_central-heating-return.csv\n",
      "home308_kitchen2831_sensor19968_tempprobe_central-heating-flow.csv\n"
     ]
    }
   ],
   "source": [
    "# Unzip and extract the content of the .csv.gz household sensor sample data\n",
    "with zf.ZipFile(hh_file_path,\"r\") as hhzip:\n",
    "    hhzip.extractall(hh_unzip_loc) \n",
    "    \n",
    "# list the files in location if the files match hom308 amd are .gz. Check subfolders with recursive=True\n",
    "all_files = glob.glob(f\"{hh_unzip_loc}/**/home308*.gz\", recursive=True)# example only interested in home308 files\n",
    "\n",
    "# iterate over the extracted file location\n",
    "for folder in all_files:\n",
    "    extract_hh_csv=os.path.splitext(os.path.basename(folder))[0] # extract file basename without extension\n",
    "    \n",
    "    # open the .gz file\n",
    "    with gzip.open(folder, 'rb') as input_gz, open(os.path.join(hh_csv_loc, extract_hh_csv), 'wb') as output_csv:\n",
    "        shutil.copyfileobj(input_gz, output_csv) # extracted csv files copied into hh_csv_loc\n",
    "\n",
    "# iterate over the files in hh_csv_loc and print their names       \n",
    "for each in [file for file in os.listdir(hh_csv_loc) if file.endswith('.csv')]:\n",
    "    print(each) # printing the household sensor filenames in hh_csv_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+\n",
      "|                _c0|_c1|\n",
      "+-------------------+---+\n",
      "|2018-03-07 13:35:17|316|\n",
      "|2018-03-07 13:41:04|316|\n",
      "|2018-03-07 13:41:52|316|\n",
      "|2018-03-07 13:42:36|316|\n",
      "|2018-03-07 13:43:17|316|\n",
      "+-------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the sensor data csv with .show() method after reading the csv\n",
    "preview_19926 = spark.read.csv(hh_csv_loc +\"/home308_outside2829_sensor19926_gas-pulse_gas.csv\")\n",
    "\n",
    "preview_19926.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------------------------------------------------+\n",
      "|datetime           |value|csv_filename                                     |\n",
      "+-------------------+-----+-------------------------------------------------+\n",
      "|2018-03-07 13:35:17|316  |home308_outside2829_sensor19926_gas-pulse_gas.csv|\n",
      "|2018-03-07 13:41:04|316  |home308_outside2829_sensor19926_gas-pulse_gas.csv|\n",
      "|2018-03-07 13:41:52|316  |home308_outside2829_sensor19926_gas-pulse_gas.csv|\n",
      "|2018-03-07 13:42:36|316  |home308_outside2829_sensor19926_gas-pulse_gas.csv|\n",
      "|2018-03-07 13:43:17|316  |home308_outside2829_sensor19926_gas-pulse_gas.csv|\n",
      "+-------------------+-----+-------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define column and names to add to the dataframe\n",
    "# Create spark dataframes from the household sensor csv\n",
    "# Create a new column and populate with the source csv file name\n",
    "# Integrate the readings from the sensors for home 308 in the example\n",
    "\n",
    "# names of files in hh_csv_loc ending in .csv\n",
    "hh_csv_list = [file for file in os.listdir(hh_csv_loc) if file.endswith('.csv')] # list of the csv filenames\n",
    "\n",
    "# initialize the variable spark_hh_dataframe to None\n",
    "spark_hh_dataframe = None\n",
    "\n",
    "# iterate over the file name in the list hh_csv_list\n",
    "# TimestampType --> timestamp datatype, IntegerType --> integer datatype\n",
    "for all_file in hh_csv_list:\n",
    "    column = StructType([StructField(\"datetime\", TimestampType(), True),\\\n",
    "                         StructField(\"value\", IntegerType(), True),])\n",
    "    \n",
    "    # create Spark dataframe with the datatype schema defined with StructType, StructField \n",
    "    each_file = spark.read.csv(os.path.join(hh_csv_loc,all_file), schema=column)\n",
    "    \n",
    "    #Extract the csv filename and use it to fill a new feature csv_filename\n",
    "    each_file = each_file.withColumn(\"csv_filename\", regexp_extract(input_file_name(), \"[^/]*$\",0)) \n",
    "    \n",
    "    # Assign each dataframe to the initialized variable and union the dataframes afterwards \n",
    "    if spark_hh_dataframe is None:\n",
    "        spark_hh_dataframe = each_file\n",
    "    else:\n",
    "        spark_hh_dataframe=spark_hh_dataframe.union(each_file) # create a union of the files\n",
    "\n",
    "spark_hh_dataframe.show(5, truncate=False) # truncate=False to view the full entry each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+-----------+----------+-------+\n",
      "|homeid|provenance|provenancedetail|energytype |date      |reading|\n",
      "+------+----------+----------------+-----------+----------+-------+\n",
      "|77    |technician|repair_visit    |electricity|2018-01-16|30561.0|\n",
      "|77    |technician|repair_visit    |gas        |2018-01-16|8081.0 |\n",
      "|79    |technician|repair_visit    |electricity|2018-01-16|28822.0|\n",
      "|79    |technician|repair_visit    |gas        |2018-01-16|5152.0 |\n",
      "|96    |technician|repair_visit    |electricity|2018-01-15|18532.0|\n",
      "+------+----------+----------------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the metadata csv with .show() method after reading the csv\n",
    "\n",
    "# path to the unzipped metadata csv output\n",
    "mtdt_csv_loc = '/home/abcxyz/userid123/metadata_and_surveys/metadata'\n",
    "\n",
    "# metadata csv filenames\n",
    "mtdt_csv_list = [file for file in os.listdir(mtdt_csv_loc) if file.endswith('.csv')] \n",
    "\n",
    "# inferSchema = True to preserve the schema of the dataset\n",
    "preview_meterreading= spark.read.csv(mtdt_csv_loc +\"/meterreading.csv\", header=True, inferSchema=True)\n",
    "\n",
    "preview_meterreading.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homeid</th>\n",
       "      <th>provenance</th>\n",
       "      <th>provenancedetail</th>\n",
       "      <th>energytype</th>\n",
       "      <th>date</th>\n",
       "      <th>reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>746.000000</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>746</td>\n",
       "      <td>7.460000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>125</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technician</td>\n",
       "      <td>installation_visit</td>\n",
       "      <td>electricity</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>425</td>\n",
       "      <td>393</td>\n",
       "      <td>374</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>211.453083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.486382e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>70.347468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.255496e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.268000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>157.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.936000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>216.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.727600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>266.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.195900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>335.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.293699e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            homeid  provenance    provenancedetail   energytype        date  \\\n",
       "count   746.000000         746                 746          746         746   \n",
       "unique         NaN         125                   4            2         163   \n",
       "top            NaN  technician  installation_visit  electricity  2018-05-23   \n",
       "freq           NaN         425                 393          374          40   \n",
       "mean    211.453083         NaN                 NaN          NaN         NaN   \n",
       "std      70.347468         NaN                 NaN          NaN         NaN   \n",
       "min      62.000000         NaN                 NaN          NaN         NaN   \n",
       "25%     157.000000         NaN                 NaN          NaN         NaN   \n",
       "50%     216.000000         NaN                 NaN          NaN         NaN   \n",
       "75%     266.000000         NaN                 NaN          NaN         NaN   \n",
       "max     335.000000         NaN                 NaN          NaN         NaN   \n",
       "\n",
       "             reading  \n",
       "count   7.460000e+02  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean    1.486382e+05  \n",
       "std     1.255496e+06  \n",
       "min     1.268000e+00  \n",
       "25%     5.936000e+03  \n",
       "50%     1.727600e+04  \n",
       "75%     4.195900e+04  \n",
       "max     2.293699e+07  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some statistics of the dataset, using the preview_meterreading example\n",
    "preview_meterreading.toPandas().describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "homeid              0\n",
       "provenance          0\n",
       "provenancedetail    0\n",
       "energytype          0\n",
       "date                0\n",
       "reading             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_meterreading.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 20:00:07 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame: spark_home\n",
      "+------+------------+---------+---------+----------------+------------------+----------------+------------------+------------------+-----------+-----------------+------------------+---------+-----------------+-----------------+-----------+--------------+--------------+----------------+-------------+---------------+-----------+--------------------+--------------+\n",
      "|homeid|install_type| location|residents|       starttime|starttime_enhanced|         endtime|          cohortid|       income_band|study_class|         hometype|equivalised_income|occupancy|urban_rural_class| urban_rural_name|  build_era|new_build_year|smart_monitors|smart_automation|occupied_days|occupied_nights|entry_floor|       outdoor_space|outdoor_drying|\n",
      "+------+------------+---------+---------+----------------+------------------+----------------+------------------+------------------+-----------+-----------------+------------------+---------+-----------------+-----------------+-----------+--------------+--------------+----------------+-------------+---------------+-----------+--------------------+--------------+\n",
      "|    47|    standard|Edinburgh|        2|10/08/2016 00:00|              NULL|08/05/2017 09:00|treatment_20170315|           Missing|  treatment|             flat|           missing| multiple|                1|Large Urban Areas|  1900-1918|          NULL|     Don't own|       Don't own|            2|              7|        3rd|                  No|          NULL|\n",
      "|    59|    standard|Edinburgh|        2|06/10/2016 00:00|              NULL|28/01/2018 07:56|  control_20170315|   £90,000 or more|    control|             flat|      above_median| multiple|                1|Large Urban Areas|  1900-1918|          NULL|     Don't own|       Don't own|            3|              7|     Ground|Yes - shared with...|           Yes|\n",
      "|    61|    enhanced|Edinburgh|        2|06/10/2016 00:00|  17/11/2016 00:00|19/10/2017 07:00| enhanced_20170213|£48,600 to £53,999|   enhanced|house_or_bungalow|      above_median| multiple|                1|Large Urban Areas|  1919-1930|          NULL|     Don't own|       Don't own|            2|              7|     Ground|Yes - shared with...|           Yes|\n",
      "|    62|    enhanced|Edinburgh|        2|26/08/2016 00:00|  11/10/2016 00:00|30/06/2018 22:59| enhanced_20170213|£43,200 to £48,599|   enhanced|             flat|      above_median| multiple|                1|Large Urban Areas|  1850-1899|          NULL|     Don't own|       Don't own|            1|              7|        2nd|Yes - shared with...|           Yes|\n",
      "|    64|    standard|Edinburgh|        4|20/10/2016 00:00|              NULL|30/06/2018 22:59|treatment_20170315|£66,000 to £77,999|  treatment|             flat|      above_median| multiple|                1|Large Urban Areas|Before 1850|          NULL|     Don't own|       Don't own|            6|              7|        1st|Yes - shared with...|           Yes|\n",
      "+------+------------+---------+---------+----------------+------------------+----------------+------------------+------------------+-----------+-----------------+------------------+---------+-----------------+-----------------+-----------+--------------+--------------+----------------+-------------+---------------+-----------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_sensor\n",
      "+--------+---------------+-----------+-----+------+------+---------------+-------------+-------+-------+\n",
      "|sensorid|    sensorboxid|       type| unit|status|roomid|subcircuit_type|scalingfactor|rawunit|counter|\n",
      "+--------+---------------+-----------+-----+------+------+---------------+-------------+-------+-------+\n",
      "|    1174|279492261292874|      light|0.1cd|active|   650|           NULL|          1.0|   NULL|      1|\n",
      "|    1175|279492261292874|   humidity| 0.1%|active|   650|           NULL|          1.0|   NULL|      1|\n",
      "|    1176|279492261292874|temperature| 0.1C|active|   650|           NULL|          1.0|   NULL|      1|\n",
      "|    1177|279492261292874|    battery|0.01V|active|   650|           NULL|          1.0|   NULL|      1|\n",
      "|    1178|279492261292874|    battery|0.01V|active|   650|           NULL|          1.0|   NULL|      2|\n",
      "+--------+---------------+-----------+-----+------+------+---------------+-------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_person\n",
      "+--------+------+------------------+--------------------+------+-------+-------------+-----------------+--------------------+-------------------+--------+----------+--------------+\n",
      "|personid|homeid|primaryparticipant|   relationtoprimary|gender|ageband|workingstatus|weeklyhoursofwork|           education|ageleavingeducation|signedup| startdate|highest_earner|\n",
      "+--------+------+------------------+--------------------+------+-------+-------------+-----------------+--------------------+-------------------+--------+----------+--------------+\n",
      "|     617|    47|                 1|                NULL|Female|  25-29|    Paid work|            31-40|Degree level qual...|                 23|       1|2016-08-16|             0|\n",
      "|     618|    47|                 0|Husband, wife or ...|  Male|  20-24|    Paid work|            41-50|Degree level qual...|               NULL|       0|2016-08-16|             0|\n",
      "|     621|    59|                 1|                NULL|  Male|  30-34|    Paid work|            31-40|Degree level qual...|                 21|       1|2016-10-06|             1|\n",
      "|     622|    59|                 0|Husband, wife or ...|Female|  30-34|    Paid work|            41-50|Degree level qual...|                 22|       0|2016-10-06|             0|\n",
      "|    1047|    59|                 0|Son or daughter, ...|  NULL|    0-4|         NULL|             NULL|                NULL|               NULL|       0|2017-09-22|             0|\n",
      "+--------+------+------------------+--------------------+------+-------+-------------+-----------------+--------------------+-------------------+--------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_room\n",
      "+------+------+----------+-------------+------+---------------+-------------+-------------+---------+------+---------+----+-------------+-----------+----------+---------+-------+-----------+---------+-------------+---------+\n",
      "|roomid|homeid|      type|secondarytype|storey|externalwindows|externaldoors|externalwalls|floorarea|height|radiators|trvs|clothesdrying|windowsopen|thermostat|othertype|stairup|stairupdoor|stairdown|stairdowndoor|mezzanine|\n",
      "+------+------+----------+-------------+------+---------------+-------------+-------------+---------+------+---------+----+-------------+-----------+----------+---------+-------+-----------+---------+-------------+---------+\n",
      "|   650|    47|   kitchen|         NULL|     3|              1|            0|            1|       55|   290|        1|None|        never|          1|         0|     NULL|      0|          0|        0|            0|        0|\n",
      "|   651|    47|livingroom|         NULL|     3|              1|            0|            1|      172|   290|        1|None|    sometimes|          1|         0|     NULL|      0|          0|        0|            0|        0|\n",
      "|   652|    47|  bathroom|         NULL|     3|              0|            0|            0|       41|   210|        1|None|        never|          0|         0|     NULL|      0|          0|        0|            0|        0|\n",
      "|   653|    47|   bedroom|         NULL|     3|              1|            0|            1|      130|   190|        1|None|        never|          1|         0|     NULL|      0|          0|        0|            0|        0|\n",
      "|   654|    47|      hall|         NULL|     3|              0|            0|            0|       29|   190|        0|None|        never|          0|         1|     NULL|      0|          0|        0|            0|        0|\n",
      "+------+------+----------+-------------+------+---------------+-------------+-------------+---------+------+---------+----+-------------+-----------+----------+---------+-------+-----------+---------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_tariff\n",
      "+------+-----------------+--------------------+-----------+---------------------------+-------------------------+\n",
      "|homeid|notification_date|    provenancedetail| energytype|daily_standing_charge_pence|unit_charge_pence_per_kwh|\n",
      "+------+-----------------+--------------------+-----------+---------------------------+-------------------------+\n",
      "|   167|       2016-08-25|primary_facetofac...|electricity|                      23.02|                     NULL|\n",
      "|    62|       2016-08-26|primary_facetofac...|electricity|                      31.31|                     NULL|\n",
      "|    62|       2016-08-26|primary_facetofac...|        gas|                      24.74|                     NULL|\n",
      "|    61|       2016-10-06|primary_facetofac...|electricity|                       0.33|                     0.12|\n",
      "|    61|       2016-10-06|primary_facetofac...|        gas|                        0.3|                     0.33|\n",
      "+------+-----------------+--------------------+-----------+---------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_other_appliance\n",
      "+----------------+------+--------------------+------+\n",
      "|otherapplianceid|homeid|      appliance_name|number|\n",
      "+----------------+------+--------------------+------+\n",
      "|               1|   113|outdoor_electric_...|     1|\n",
      "|               2|   126|outdoor_gas_space...|     2|\n",
      "|               3|   306|outdoor_gas_space...|     2|\n",
      "|               4|   168|     outdoor_hot_tub|     1|\n",
      "|               5|    89|outdoor_water_fea...|     1|\n",
      "+----------------+------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_appliance\n",
      "+-----------+------+------+--------------+---------+-------------+--------------------+------+\n",
      "|applianceid|homeid|roomid|applianceclass|powertype|appliancetype|    appliancesubtype|number|\n",
      "+-----------+------+------+--------------+---------+-------------+--------------------+------+\n",
      "|        907|    47|   650|          food|      gas|       gashob|      gasCookingHobs|     4|\n",
      "|        908|    47|   650|          food| electric|fridgefreezer|combinedFridgeFre...|     1|\n",
      "|        909|    47|   650|          food| electric|        grill|               grill|     1|\n",
      "|        910|    47|   650|          food| electric|      toaster|             toaster|     1|\n",
      "|        911|    47|   650|          food| electric|       kettle|              kettle|     1|\n",
      "+-----------+------+------+--------------+---------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_sensorbox\n",
      "+---------------+--------+------+------+--------------+------------+---------------+----+----------------+---------------------+------------------+---------------+------------+------------+----------+----------+--------+-----------+-----------+------+------------+------------+----+--------+\n",
      "|    sensorboxid|local_id|roomid|status|sensorbox_type|       notes|heightfromfloor|name|onMainThermostat|temperatureInaccuracy|humidityInaccuracy|lightInaccuracy|install_type|currentrange|clamp1pipe|clamp2pipe|gasblock|installtime|applianceid|hasTRV|clamp1detail|clamp2detail|oven|function|\n",
      "+---------------+--------+------+------+--------------+------------+---------------+----+----------------+---------------------+------------------+---------------+------------+------------+----------+----------+--------+-----------+-----------+------+------------+------------+----+--------+\n",
      "|279492261292874|       1|   650|active|          room|Behind door.|            157|NULL|               0|                    0|                 0|              1|    standard|        NULL|      NULL|      NULL|    NULL| 1470819942|          0|  NULL|        NULL|        NULL|NULL|    NULL|\n",
      "|279492261292875|       2|   650|active|         clamp|        NULL|              0|NULL|               0|                    0|                 0|              0|    standard|        NULL|      Cold|       Hot|    NULL| 1470821130|          0|  NULL|        NULL|        NULL|NULL|    NULL|\n",
      "|279492261292876|       3|   650|active|         clamp|        NULL|              0|NULL|               0|                    0|                 0|              0|    standard|        NULL| CH Return|   CH Flow|    NULL| 1470821500|          0|  NULL|        NULL|        NULL|NULL|    NULL|\n",
      "|279492261292877|       4|   651|active|          room|        NULL|            161|NULL|               0|                    0|                 0|              0|    standard|        NULL|      NULL|      NULL|    NULL| 1470822382|          0|  NULL|        NULL|        NULL|NULL|    NULL|\n",
      "|279492261292878|       5|   652|active|          room|        NULL|            158|NULL|               0|                    0|                 0|              0|    standard|        NULL|      NULL|      NULL|    NULL| 1470822890|          0|  NULL|        NULL|        NULL|NULL|    NULL|\n",
      "+---------------+--------+------+------+--------------+------------+---------------+----+----------------+---------------------+------------------+---------------+------------+------------+----------+----------+--------+-----------+-----------+------+------------+------------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_weatherfeed\n",
      "+------+-------------+----------+------+--------------------+--------------------+\n",
      "|feedid| weather_type|locationid|  unit|              source|                 url|\n",
      "+------+-------------+----------+------+--------------------+--------------------+\n",
      "|     1|  temperature| Edinburgh|  0.1C|Weather Undergrou...|http://api.wunder...|\n",
      "|     2|   conditions| Edinburgh|  text|Weather Undergrou...|http://api.wunder...|\n",
      "|     3|     humidity| Edinburgh|  0.1%|Weather Undergrou...|http://api.wunder...|\n",
      "|     4|    windspeed| Edinburgh|0.1kph|Weather Undergrou...|http://api.wunder...|\n",
      "|     5|winddirection| Edinburgh|  NULL|Weather Undergrou...|http://api.wunder...|\n",
      "+------+-------------+----------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_meterreading\n",
      "+------+----------+----------------+-----------+----------+-------+\n",
      "|homeid|provenance|provenancedetail| energytype|      date|reading|\n",
      "+------+----------+----------------+-----------+----------+-------+\n",
      "|    77|technician|    repair_visit|electricity|2018-01-16|30561.0|\n",
      "|    77|technician|    repair_visit|        gas|2018-01-16| 8081.0|\n",
      "|    79|technician|    repair_visit|electricity|2018-01-16|28822.0|\n",
      "|    79|technician|    repair_visit|        gas|2018-01-16| 5152.0|\n",
      "|    96|technician|    repair_visit|electricity|2018-01-15|18532.0|\n",
      "+------+----------+----------------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark DataFrame: spark_location\n",
      "+-----------+--------------+\n",
      "| locationid|weather_centre|\n",
      "+-----------+--------------+\n",
      "|  Edinburgh|     Edinburgh|\n",
      "|WestLothian|    Livingston|\n",
      "| Midlothian|      Penicuik|\n",
      "|EastLothian| North Berwick|\n",
      "|       Fife|     Kirkcaldy|\n",
      "+-----------+--------------+\n",
      "\n",
      "Spark Schema: spark_home\n",
      "root\n",
      " |-- homeid: integer (nullable = true)\n",
      " |-- install_type: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- residents: integer (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- starttime_enhanced: string (nullable = true)\n",
      " |-- endtime: string (nullable = true)\n",
      " |-- cohortid: string (nullable = true)\n",
      " |-- income_band: string (nullable = true)\n",
      " |-- study_class: string (nullable = true)\n",
      " |-- hometype: string (nullable = true)\n",
      " |-- equivalised_income: string (nullable = true)\n",
      " |-- occupancy: string (nullable = true)\n",
      " |-- urban_rural_class: string (nullable = true)\n",
      " |-- urban_rural_name: string (nullable = true)\n",
      " |-- build_era: string (nullable = true)\n",
      " |-- new_build_year: integer (nullable = true)\n",
      " |-- smart_monitors: string (nullable = true)\n",
      " |-- smart_automation: string (nullable = true)\n",
      " |-- occupied_days: integer (nullable = true)\n",
      " |-- occupied_nights: integer (nullable = true)\n",
      " |-- entry_floor: string (nullable = true)\n",
      " |-- outdoor_space: string (nullable = true)\n",
      " |-- outdoor_drying: string (nullable = true)\n",
      "\n",
      "Spark Schema: spark_sensor\n",
      "root\n",
      " |-- sensorid: integer (nullable = true)\n",
      " |-- sensorboxid: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- roomid: integer (nullable = true)\n",
      " |-- subcircuit_type: string (nullable = true)\n",
      " |-- scalingfactor: double (nullable = true)\n",
      " |-- rawunit: string (nullable = true)\n",
      " |-- counter: integer (nullable = true)\n",
      "\n",
      "Spark Schema: spark_person\n",
      "root\n",
      " |-- personid: integer (nullable = true)\n",
      " |-- homeid: integer (nullable = true)\n",
      " |-- primaryparticipant: integer (nullable = true)\n",
      " |-- relationtoprimary: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ageband: string (nullable = true)\n",
      " |-- workingstatus: string (nullable = true)\n",
      " |-- weeklyhoursofwork: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- ageleavingeducation: integer (nullable = true)\n",
      " |-- signedup: integer (nullable = true)\n",
      " |-- startdate: date (nullable = true)\n",
      " |-- highest_earner: integer (nullable = true)\n",
      "\n",
      "Spark Schema: spark_room\n",
      "root\n",
      " |-- roomid: integer (nullable = true)\n",
      " |-- homeid: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- secondarytype: string (nullable = true)\n",
      " |-- storey: integer (nullable = true)\n",
      " |-- externalwindows: integer (nullable = true)\n",
      " |-- externaldoors: integer (nullable = true)\n",
      " |-- externalwalls: integer (nullable = true)\n",
      " |-- floorarea: integer (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- radiators: integer (nullable = true)\n",
      " |-- trvs: string (nullable = true)\n",
      " |-- clothesdrying: string (nullable = true)\n",
      " |-- windowsopen: integer (nullable = true)\n",
      " |-- thermostat: integer (nullable = true)\n",
      " |-- othertype: string (nullable = true)\n",
      " |-- stairup: integer (nullable = true)\n",
      " |-- stairupdoor: integer (nullable = true)\n",
      " |-- stairdown: integer (nullable = true)\n",
      " |-- stairdowndoor: integer (nullable = true)\n",
      " |-- mezzanine: integer (nullable = true)\n",
      "\n",
      "Spark Schema: spark_tariff\n",
      "root\n",
      " |-- homeid: integer (nullable = true)\n",
      " |-- notification_date: date (nullable = true)\n",
      " |-- provenancedetail: string (nullable = true)\n",
      " |-- energytype: string (nullable = true)\n",
      " |-- daily_standing_charge_pence: double (nullable = true)\n",
      " |-- unit_charge_pence_per_kwh: double (nullable = true)\n",
      "\n",
      "Spark Schema: spark_other_appliance\n",
      "root\n",
      " |-- otherapplianceid: integer (nullable = true)\n",
      " |-- homeid: integer (nullable = true)\n",
      " |-- appliance_name: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      "\n",
      "Spark Schema: spark_appliance\n",
      "root\n",
      " |-- applianceid: integer (nullable = true)\n",
      " |-- homeid: integer (nullable = true)\n",
      " |-- roomid: integer (nullable = true)\n",
      " |-- applianceclass: string (nullable = true)\n",
      " |-- powertype: string (nullable = true)\n",
      " |-- appliancetype: string (nullable = true)\n",
      " |-- appliancesubtype: string (nullable = true)\n",
      " |-- number: integer (nullable = true)\n",
      "\n",
      "Spark Schema: spark_sensorbox\n",
      "root\n",
      " |-- sensorboxid: long (nullable = true)\n",
      " |-- local_id: integer (nullable = true)\n",
      " |-- roomid: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- sensorbox_type: string (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      " |-- heightfromfloor: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- onMainThermostat: integer (nullable = true)\n",
      " |-- temperatureInaccuracy: integer (nullable = true)\n",
      " |-- humidityInaccuracy: integer (nullable = true)\n",
      " |-- lightInaccuracy: integer (nullable = true)\n",
      " |-- install_type: string (nullable = true)\n",
      " |-- currentrange: string (nullable = true)\n",
      " |-- clamp1pipe: string (nullable = true)\n",
      " |-- clamp2pipe: string (nullable = true)\n",
      " |-- gasblock: string (nullable = true)\n",
      " |-- installtime: integer (nullable = true)\n",
      " |-- applianceid: integer (nullable = true)\n",
      " |-- hasTRV: integer (nullable = true)\n",
      " |-- clamp1detail: string (nullable = true)\n",
      " |-- clamp2detail: string (nullable = true)\n",
      " |-- oven: string (nullable = true)\n",
      " |-- function: string (nullable = true)\n",
      "\n",
      "Spark Schema: spark_weatherfeed\n",
      "root\n",
      " |-- feedid: integer (nullable = true)\n",
      " |-- weather_type: string (nullable = true)\n",
      " |-- locationid: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n",
      "Spark Schema: spark_meterreading\n",
      "root\n",
      " |-- homeid: integer (nullable = true)\n",
      " |-- provenance: string (nullable = true)\n",
      " |-- provenancedetail: string (nullable = true)\n",
      " |-- energytype: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- reading: double (nullable = true)\n",
      "\n",
      "Spark Schema: spark_location\n",
      "root\n",
      " |-- locationid: string (nullable = true)\n",
      " |-- weather_centre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate over the csv in the folder to create spark dataframes\n",
    "# prefix each dataframe with spark_ in addition to the source csv file\n",
    "\n",
    "\n",
    "spark_prefix = \"spark_\"\n",
    "\n",
    "# initialize the variable spark_df\n",
    "spark_df = {}\n",
    "\n",
    "# loop over the mtdt_csv_list and create Spark dataframe from each csv in the folder\n",
    "for all_metadata_file in mtdt_csv_list:\n",
    "    metadata_file = spark.read.csv(os.path.join(mtdt_csv_loc,all_metadata_file), header=True, inferSchema=True)\n",
    "    each_mtdt_filename = spark_prefix+os.path.splitext(all_metadata_file)[0]\n",
    "    spark_df[each_mtdt_filename] = metadata_file\n",
    "\n",
    "\n",
    "# loop over Spark dataframe created in the previous loop and display the first 5 rows of the dataframe\n",
    "for mtdt_fn, each_metadata_file in spark_df.items():\n",
    "    each_metadata_file\n",
    "    print(f\"Spark DataFrame: {mtdt_fn}\")\n",
    "    each_metadata_file.show(5)\n",
    "    \n",
    "# loop over Spark dataframe created in the previous loop and print the schema    \n",
    "for mtdt_fn, each_metadata_file in spark_df.items():\n",
    "    print(f\"Spark Schema: {mtdt_fn}\")\n",
    "    each_metadata_file.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data types for some datetime columns from string to a format of timestamp within spark_home dataframe\n",
    "\n",
    "# assign some columns of interest to the variable timestampcol_spark_home\n",
    "timestampcol_spark_home = [\"starttime\", \"starttime_enhanced\", \"endtime\"]\n",
    "col_format = \"dd/MM/yyyy HH:mm\"\n",
    "\n",
    "# loop over the listed features in the variable timestampcol_spark_home and perform the datatype change\n",
    "for colname in timestampcol_spark_home:\n",
    "    spark_df['spark_home']=spark_df['spark_home'].withColumn(colname, to_timestamp(colname,col_format))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- homeid: integer (nullable = true)\n",
      " |-- install_type: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- residents: integer (nullable = true)\n",
      " |-- starttime: timestamp (nullable = true)\n",
      " |-- starttime_enhanced: timestamp (nullable = true)\n",
      " |-- endtime: timestamp (nullable = true)\n",
      " |-- cohortid: string (nullable = true)\n",
      " |-- income_band: string (nullable = true)\n",
      " |-- study_class: string (nullable = true)\n",
      " |-- hometype: string (nullable = true)\n",
      " |-- equivalised_income: string (nullable = true)\n",
      " |-- occupancy: string (nullable = true)\n",
      " |-- urban_rural_class: string (nullable = true)\n",
      " |-- urban_rural_name: string (nullable = true)\n",
      " |-- build_era: string (nullable = true)\n",
      " |-- new_build_year: integer (nullable = true)\n",
      " |-- smart_monitors: string (nullable = true)\n",
      " |-- smart_automation: string (nullable = true)\n",
      " |-- occupied_days: integer (nullable = true)\n",
      " |-- occupied_nights: integer (nullable = true)\n",
      " |-- entry_floor: string (nullable = true)\n",
      " |-- outdoor_space: string (nullable = true)\n",
      " |-- outdoor_drying: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the schema to see if the changes took place \n",
    "spark_df['spark_home'].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark_home',\n",
       " 'spark_sensor',\n",
       " 'spark_person',\n",
       " 'spark_room',\n",
       " 'spark_tariff',\n",
       " 'spark_other_appliance',\n",
       " 'spark_appliance',\n",
       " 'spark_sensorbox',\n",
       " 'spark_weatherfeed',\n",
       " 'spark_meterreading',\n",
       " 'spark_location']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieves a list of the keys in the dictionary\n",
    "list(spark_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create appropraite names for the spark dataframes\n",
    "spark_home=spark_df['spark_home']\n",
    "spark_sensor=spark_df['spark_sensor']\n",
    "spark_person=spark_df['spark_person']\n",
    "spark_room=spark_df['spark_room']\n",
    "spark_tariff=spark_df['spark_tariff']\n",
    "spark_other_appliance=spark_df['spark_other_appliance']\n",
    "spark_appliance=spark_df['spark_appliance']\n",
    "spark_sensorbox=spark_df['spark_sensorbox']\n",
    "spark_weatherfeed=spark_df['spark_weatherfeed']\n",
    "spark_meterreading=spark_df['spark_meterreading']\n",
    "spark_location=spark_df['spark_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 20:00:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homeid</th>\n",
       "      <td>746</td>\n",
       "      <td>211.45308310991956</td>\n",
       "      <td>70.34746798731733</td>\n",
       "      <td>62</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provenance</th>\n",
       "      <td>746</td>\n",
       "      <td>972.4205607476636</td>\n",
       "      <td>208.8933057829631</td>\n",
       "      <td>1007</td>\n",
       "      <td>technician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provenancedetail</th>\n",
       "      <td>746</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>all_inapp_meters_mid</td>\n",
       "      <td>repair_visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energytype</th>\n",
       "      <td>746</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>electricity</td>\n",
       "      <td>gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reading</th>\n",
       "      <td>746</td>\n",
       "      <td>148638.2009155496</td>\n",
       "      <td>1255496.1026341487</td>\n",
       "      <td>1.268</td>\n",
       "      <td>2.2936994E7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                   1                   2  \\\n",
       "summary           count                mean              stddev   \n",
       "homeid              746  211.45308310991956   70.34746798731733   \n",
       "provenance          746   972.4205607476636   208.8933057829631   \n",
       "provenancedetail    746                None                None   \n",
       "energytype          746                None                None   \n",
       "reading             746   148638.2009155496  1255496.1026341487   \n",
       "\n",
       "                                     3             4  \n",
       "summary                            min           max  \n",
       "homeid                              62           335  \n",
       "provenance                        1007    technician  \n",
       "provenancedetail  all_inapp_meters_mid  repair_visit  \n",
       "energytype                 electricity           gas  \n",
       "reading                          1.268   2.2936994E7  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some statistics of the dataset, using \n",
    "spark_meterreading.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensorid</th>\n",
       "      <td>20081</td>\n",
       "      <td>12037.284198994073</td>\n",
       "      <td>6266.531893731428</td>\n",
       "      <td>1174</td>\n",
       "      <td>31573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensorboxid</th>\n",
       "      <td>20081</td>\n",
       "      <td>2.794922612950086E14</td>\n",
       "      <td>1172.6833868025014</td>\n",
       "      <td>279492261292874</td>\n",
       "      <td>279492261297177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>20081</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>battery</td>\n",
       "      <td>temperature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>20081</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01V</td>\n",
       "      <td>1Wh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <td>20081</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>active</td>\n",
       "      <td>offline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roomid</th>\n",
       "      <td>20081</td>\n",
       "      <td>1856.3234400677256</td>\n",
       "      <td>682.543379466405</td>\n",
       "      <td>650</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subcircuit_type</th>\n",
       "      <td>86</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>attic plug socket - marked as shower</td>\n",
       "      <td>water_heater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scalingfactor</th>\n",
       "      <td>20081</td>\n",
       "      <td>0.950398790767392</td>\n",
       "      <td>0.21467270601832275</td>\n",
       "      <td>0.00911</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rawunit</th>\n",
       "      <td>1288</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01m3</td>\n",
       "      <td>1ft3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counter</th>\n",
       "      <td>20081</td>\n",
       "      <td>1.2648274488322295</td>\n",
       "      <td>0.4432791397893889</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                     1                    2  \\\n",
       "summary          count                  mean               stddev   \n",
       "sensorid         20081    12037.284198994073    6266.531893731428   \n",
       "sensorboxid      20081  2.794922612950086E14   1172.6833868025014   \n",
       "type             20081                  None                 None   \n",
       "unit             20081                  None                 None   \n",
       "status           20081                  None                 None   \n",
       "roomid           20081    1856.3234400677256     682.543379466405   \n",
       "subcircuit_type     86                  None                 None   \n",
       "scalingfactor    20081     0.950398790767392  0.21467270601832275   \n",
       "rawunit           1288                  None                 None   \n",
       "counter          20081    1.2648274488322295   0.4432791397893889   \n",
       "\n",
       "                                                    3                4  \n",
       "summary                                           min              max  \n",
       "sensorid                                         1174            31573  \n",
       "sensorboxid                           279492261292874  279492261297177  \n",
       "type                                          battery      temperature  \n",
       "unit                                            0.01V              1Wh  \n",
       "status                                         active          offline  \n",
       "roomid                                            650             3071  \n",
       "subcircuit_type  attic plug socket - marked as shower     water_heater  \n",
       "scalingfactor                                 0.00911              1.0  \n",
       "rawunit                                        0.01m3             1ft3  \n",
       "counter                                             1                4  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain the statistics of the Spark dataframe, convert the result to Pandas and transpose the output \n",
    "spark_sensor.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+----+------+---+\n",
      "|feedid|weather_type|locationid|unit|source|url|\n",
      "+------+------------+----------+----+------+---+\n",
      "+------+------------+----------+----+------+---+\n",
      "\n",
      "+------+----------+----------------+----------+----+-------+\n",
      "|homeid|provenance|provenancedetail|energytype|date|reading|\n",
      "+------+----------+----------------+----------+----+-------+\n",
      "+------+----------+----------------+----------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates - in few examples\n",
    "\n",
    "spark_weatherfeed.exceptAll(spark_weatherfeed.dropDuplicates()).show()\n",
    "spark_meterreading.exceptAll(spark_meterreading.dropDuplicates()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1790955"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check record count in the household sensor data \n",
    "# Check for and remove duplicates, then reconfirm record count\n",
    "spark_hh_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----------------------------------------------------------------------------+\n",
      "|datetime           |value|csv_filename                                                                 |\n",
      "+-------------------+-----+-----------------------------------------------------------------------------+\n",
      "|2018-03-25 02:57:55|131  |home308_livingroom2828_sensor19920c19924_electric-mains_electric-combined.csv|\n",
      "|2018-03-25 02:57:43|131  |home308_livingroom2828_sensor19920c19924_electric-mains_electric-combined.csv|\n",
      "|2018-03-25 02:57:34|131  |home308_livingroom2828_sensor19920c19924_electric-mains_electric-combined.csv|\n",
      "|2018-03-25 02:57:26|131  |home308_livingroom2828_sensor19920c19924_electric-mains_electric-combined.csv|\n",
      "+-------------------+-----+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Allow logging output to be at ERROR level rather than several logging outputs \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# show duplicate records\n",
    "spark_hh_dataframe.exceptAll(spark_hh_dataframe.dropDuplicates()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1790951"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove duplates\n",
    "dedup_spark_hh_dataframe = spark_hh_dataframe.dropDuplicates()\n",
    "\n",
    "# replace the spark dataframe with the dedup dataframe\n",
    "spark_hh_dataframe=dedup_spark_hh_dataframe\n",
    "\n",
    "# check record count after duplicate removal\n",
    "spark_hh_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|          timestamp|value|        csv_filename|\n",
      "+-------------------+-----+--------------------+\n",
      "|2018-03-07 19:37:55| 1264|home308_outside28...|\n",
      "|2018-03-07 22:56:20| 2527|home308_outside28...|\n",
      "|2018-03-08 07:25:11|  316|home308_outside28...|\n",
      "|2018-03-14 20:41:50|  316|home308_outside28...|\n",
      "|2018-03-15 23:51:01|  316|home308_outside28...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Rename column datetime to timestamp in the spark household sensor dataframe\n",
    "spark_hh_dataframe = spark_hh_dataframe.withColumnRenamed(\"datetime\", \"timestamp\")\n",
    "\n",
    "spark_hh_dataframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(csv_filename='home308_outside2829_sensor19926_gas-pulse_gas.csv'),\n",
       " Row(csv_filename='home308_livingroom2828_sensor19920c19924_electric-mains_electric-combined.csv'),\n",
       " Row(csv_filename='home308_kitchen2831_sensor19973_tempprobe_hot-water-hot-pipe.csv'),\n",
       " Row(csv_filename='home308_kitchen2831_sensor19972_tempprobe_hot-water-cold-pipe.csv'),\n",
       " Row(csv_filename='home308_kitchen2831_sensor19967_tempprobe_central-heating-return.csv'),\n",
       " Row(csv_filename='home308_kitchen2831_sensor19968_tempprobe_central-heating-flow.csv')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to ensure the csv_filename contains the complete set of source csv filenames\n",
    "spark_hh_dataframe.select(\"csv_filename\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abcxyz/userid123/anaconda3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+----+------+------+--------------------+-------------+-------+-------+\n",
      "|sensorid|    sensorboxid| type|unit|status|roomid|     subcircuit_type|scalingfactor|rawunit|counter|\n",
      "+--------+---------------+-----+----+------+------+--------------------+-------------+-------+-------+\n",
      "|   13124|279492261295239|power|  1W|active|  2093|attic plug socket...|          1.0|   NULL|      1|\n",
      "+--------+---------------+-----+----+------+------+--------------------+-------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create temporary table with registerTempTable to perform sql like operations\n",
    "spark_sensor.registerTempTable(\"spark_sensor\")\n",
    "spark_sensor_output = spark.sql('SELECT * from spark_sensor where sensorid = 13124')\n",
    "spark_sensor_output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------------------+-----------+---------------------------+-------------------------+\n",
      "|homeid|notification_date|    provenancedetail| energytype|daily_standing_charge_pence|unit_charge_pence_per_kwh|\n",
      "+------+-----------------+--------------------+-----------+---------------------------+-------------------------+\n",
      "|   167|       2016-08-25|primary_facetofac...|electricity|                      23.02|                     NULL|\n",
      "|    62|       2016-08-26|primary_facetofac...|electricity|                      31.31|                     NULL|\n",
      "|    62|       2016-08-26|primary_facetofac...|        gas|                      24.74|                     NULL|\n",
      "|    61|       2016-10-06|primary_facetofac...|electricity|                       0.33|                     0.12|\n",
      "|    61|       2016-10-06|primary_facetofac...|        gas|                        0.3|                     0.33|\n",
      "+------+-----------------+--------------------+-----------+---------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change data in a column using spark functions when() and otherwise() \n",
    "spark_sensor = spark_sensor.withColumn(\"status\", when(col(\"status\") == \\\n",
    "                                    \"offline\", \"inactive\").otherwise(col(\"status\")))\n",
    "spark_tariff.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+-----+--------+------+---------------+-------------+-------+-------+\n",
      "|sensorid|    sensorboxid|       type| unit|  status|roomid|subcircuit_type|scalingfactor|rawunit|counter|\n",
      "+--------+---------------+-----------+-----+--------+------+---------------+-------------+-------+-------+\n",
      "|    4225|279492261293492|      light|0.1cd|inactive|  1011|           NULL|          1.0|   NULL|      1|\n",
      "|    4226|279492261293492|   humidity| 0.1%|inactive|  1011|           NULL|          1.0|   NULL|      1|\n",
      "|    4227|279492261293492|temperature| 0.1C|inactive|  1011|           NULL|          1.0|   NULL|      1|\n",
      "|    4228|279492261293492|    battery|0.01V|inactive|  1011|           NULL|          1.0|   NULL|      1|\n",
      "|    4229|279492261293492|    battery|0.01V|inactive|  1011|           NULL|          1.0|   NULL|      2|\n",
      "+--------+---------------+-----------+-----+--------+------+---------------+-------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_sensor.registerTempTable(\"spark_sensor\")\n",
    "spark_sensor_output = spark.sql('SELECT * from spark_sensor where status = \"inactive\"')\n",
    "spark_sensor_output.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the household sensor dataframe spark_hh_dataframe and join with spark_tariff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- home_id: integer (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- room_id: integer (nullable = true)\n",
      " |-- sensor_id: integer (nullable = true)\n",
      " |-- sensorbox_type: string (nullable = true)\n",
      " |-- sensor_type: string (nullable = true)\n",
      " |-- sensorid: integer (nullable = true)\n",
      " |-- sensorboxid: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- roomid: integer (nullable = true)\n",
      " |-- subcircuit_type: string (nullable = true)\n",
      " |-- scalingfactor: double (nullable = true)\n",
      " |-- rawunit: string (nullable = true)\n",
      " |-- counter: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The column to split\n",
    "col_to_split = col(\"csv_filename\")\n",
    "\n",
    "# The column shall be split into the follwing parts\n",
    "# this have been obtained from the documentation.pdf for IDEAL household energy sources\n",
    "# that referenced file naming format and the constituent parts\n",
    "col_name = [\"home_id\",\"room_type\", \"room_id\", \"sensor_id\", \"sensorbox_type\", \"sensor_type\"]\n",
    "\n",
    "# split the compound column csv_filename and assign new column name to each split\n",
    "# ignoring the homeid in index 0, because the data is from home 308 and the same id over the column\n",
    "\n",
    "spark_hh_df_split = spark_hh_dataframe\\\n",
    "            .withColumn(\"home_id\", regexp_extract(split(col_to_split, \"_\").getItem(0),r'\\d+',0).cast('INT')) \\\n",
    "            .withColumn(\"room_type\", regexp_extract(split(col_to_split, \"_\").getItem(1),r'[a-zA-Z]+',0)) \\\n",
    "            .withColumn(\"room_id\", regexp_extract(split(col_to_split, \"_\").getItem(1),r'\\d+',0).cast('INT')) \\\n",
    "            .withColumn(\"sensor_id\", regexp_extract(split(col_to_split, \"_\").getItem(2),r'\\d+',0).cast('INT')) \\\n",
    "            .withColumn(\"sensorbox_type\", split(col_to_split, \"_\").getItem(3)) \\\n",
    "            .withColumn(\"sensor_type\", regexp_replace(split(col_to_split, \"_\").getItem(4),'.csv',''))\n",
    "\n",
    "\n",
    "# Join the spark_hh_dataframe_split and spark_room metadata dataframe to get the associated\n",
    "# metadata for each room in the home. \n",
    "# Do the join on the roomid of the now split spark_hh_dataframe and the roomid of the spark_room metadata\n",
    "\n",
    "csld_spark_hh_dataframe = spark_hh_df_split\\\n",
    "            .join(spark_sensor, spark_hh_df_split.sensor_id==spark_sensor.sensorid, how='inner')\n",
    "\n",
    "\n",
    "# Drop columns from the consolidated dataframe\n",
    "csld_spark_hh_dataframe = csld_spark_hh_dataframe.drop('csv_filename','homeid','notification_date')\n",
    "\n",
    "# View the schema of the new dataframe\n",
    "csld_spark_hh_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns of interest as an option rather than drop\n",
    "csld_spark_hh_dataframe=csld_spark_hh_dataframe.select(\"timestamp\", \"value\", \"home_id\",\"room_type\", \"room_id\",\\\n",
    "                                                       \"sensor_id\", \"sensorbox_type\", \"sensor_type\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+---------+-------+---------+--------------+-----------+------+\n",
      "|          timestamp|value|home_id|room_type|room_id|sensor_id|sensorbox_type|sensor_type|status|\n",
      "+-------------------+-----+-------+---------+-------+---------+--------------+-----------+------+\n",
      "|2018-03-07 19:37:55| 1264|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|\n",
      "|2018-03-07 22:56:20| 2527|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|\n",
      "|2018-03-08 07:25:11|  316|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|\n",
      "|2018-03-14 20:41:50|  316|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|\n",
      "|2018-03-15 23:51:01|  316|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|\n",
      "+-------------------+-----+-------+---------+-------+---------+--------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csld_spark_hh_dataframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in the columns\n",
    "\n",
    "# get the missing values in the dataframe\n",
    "missing_values_df = csld_spark_hh_dataframe.selectExpr(*[f\"sum(int({col} is null)) as {col}\" \\\n",
    "                                                         for col in csld_spark_hh_dataframe.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-------+---------+-------+---------+--------------+-----------+------+\n",
      "|timestamp|value|home_id|room_type|room_id|sensor_id|sensorbox_type|sensor_type|status|\n",
      "+---------+-----+-------+---------+-------+---------+--------------+-----------+------+\n",
      "|        0|    0|      0|        0|      0|        0|             0|          0|     0|\n",
      "+---------+-----+-------+---------+-------+---------+--------------+-----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         timestamp      value    home_id   room_type  \\\n",
      "count                      1790951 1790951.00 1790951.00     1790951   \n",
      "unique                         NaN        NaN        NaN           3   \n",
      "top                            NaN        NaN        NaN  livingroom   \n",
      "freq                           NaN        NaN        NaN     1382973   \n",
      "mean    2018-03-16 13:10:10.050400     341.26     308.00         NaN   \n",
      "min            2018-03-07 13:21:30       0.00     308.00         NaN   \n",
      "25%            2018-03-12 09:07:09     133.00     308.00         NaN   \n",
      "50%            2018-03-16 13:45:56     184.00     308.00         NaN   \n",
      "75%     2018-03-20 20:01:22.500000     301.00     308.00         NaN   \n",
      "max            2018-03-25 14:44:32   94781.00     308.00         NaN   \n",
      "std                            NaN     574.13       0.00         NaN   \n",
      "\n",
      "          room_id  sensor_id  sensorbox_type        sensor_type   status  \n",
      "count  1790951.00 1790951.00         1790951            1790951  1790951  \n",
      "unique        NaN        NaN               3                  6        1  \n",
      "top           NaN        NaN  electric-mains  electric-combined   active  \n",
      "freq          NaN        NaN         1382973            1382973  1790951  \n",
      "mean      2828.68   19931.37             NaN                NaN      NaN  \n",
      "min       2828.00   19920.00             NaN                NaN      NaN  \n",
      "25%       2828.00   19920.00             NaN                NaN      NaN  \n",
      "50%       2828.00   19920.00             NaN                NaN      NaN  \n",
      "75%       2828.00   19920.00             NaN                NaN      NaN  \n",
      "max       2831.00   19973.00             NaN                NaN      NaN  \n",
      "std          1.26      21.01             NaN                NaN      NaN  \n"
     ]
    }
   ],
   "source": [
    "# Display some statistics, results without scientific notation\n",
    "with pd.option_context('display.float_format', '{:.2f}'.format):\n",
    "    stats_csld_spark_hh_dataframe = csld_spark_hh_dataframe.toPandas().describe(include='all')\n",
    "    print(stats_csld_spark_hh_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# for the categorical features, assign index values to the distinct entries\n",
    "\n",
    "# select the columns to assign index and a new name for the indexed output\n",
    "cat_features = [(\"room_type\",\"roomtype\"),(\"sensorbox_type\",\"sensorboxtype\"),\\\n",
    "                (\"sensor_type\",\"sensortype\"),(\"status\",\"sensor_status\")]\n",
    "\n",
    "# create a list, iterate over and apply index using StringIndexer to transform the features, apply assigned index\n",
    "index_cat = [StringIndexer(inputCol=col_to_input, outputCol=col_to_output)\\\n",
    "            for col_to_input, col_to_output in cat_features]\n",
    "\n",
    "# create a pipeline object to fir the StringIndexer model into the dataframe\n",
    "pipeline_index = Pipeline(stages=index_cat).fit(csld_spark_hh_dataframe)\n",
    "\n",
    "# transform the dataframe\n",
    "csld_spark_hh_dataframe = pipeline_index.transform(csld_spark_hh_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 133:==================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+---------+-------+---------+--------------+-----------+------+--------+-------------+----------+-------------+\n",
      "|          timestamp|value|home_id|room_type|room_id|sensor_id|sensorbox_type|sensor_type|status|roomtype|sensorboxtype|sensortype|sensor_status|\n",
      "+-------------------+-----+-------+---------+-------+---------+--------------+-----------+------+--------+-------------+----------+-------------+\n",
      "|2018-03-07 19:37:55| 1264|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|     2.0|          2.0|       5.0|          0.0|\n",
      "|2018-03-07 22:56:20| 2527|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|     2.0|          2.0|       5.0|          0.0|\n",
      "|2018-03-08 07:25:11|  316|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|     2.0|          2.0|       5.0|          0.0|\n",
      "|2018-03-14 20:41:50|  316|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|     2.0|          2.0|       5.0|          0.0|\n",
      "|2018-03-15 23:51:01|  316|    308|  outside|   2829|    19926|     gas-pulse|        gas|active|     2.0|          2.0|       5.0|          0.0|\n",
      "+-------------------+-----+-------+---------+-------+---------+--------------+-----------+------+--------+-------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# display the first 5 records of the new dataframe \n",
    "csld_spark_hh_dataframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the indexed column of interest\n",
    "# select columns of interest as an option rather than drop\n",
    "csld_spark_hh_dataframe=csld_spark_hh_dataframe.select(\"timestamp\", \"value\", \"home_id\",\"roomtype\", \"room_id\",\\\n",
    "                                                       \"sensor_id\", \"sensorboxtype\", \"sensortype\", \"sensor_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:==============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+\n",
      "|          timestamp|value|home_id|roomtype|room_id|sensor_id|sensorboxtype|sensortype|sensor_status|\n",
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+\n",
      "|2018-03-07 19:37:55| 1264|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0|\n",
      "|2018-03-07 22:56:20| 2527|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0|\n",
      "|2018-03-08 07:25:11|  316|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0|\n",
      "|2018-03-14 20:41:50|  316|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0|\n",
      "|2018-03-15 23:51:01|  316|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0|\n",
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# display the first 5 records of the new dataframe \n",
    "csld_spark_hh_dataframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract day, month, year, hour, minute, seconds from timestamp column\n",
    "csld_spark_hh_dataframe = csld_spark_hh_dataframe.withColumn(\"day\", dayofmonth(\"timestamp\"))\n",
    "csld_spark_hh_dataframe = csld_spark_hh_dataframe.withColumn(\"month\", month(\"timestamp\"))\n",
    "csld_spark_hh_dataframe = csld_spark_hh_dataframe.withColumn(\"year\", year(\"timestamp\"))\n",
    "csld_spark_hh_dataframe = csld_spark_hh_dataframe.withColumn(\"hour\", hour(\"timestamp\"))\n",
    "csld_spark_hh_dataframe = csld_spark_hh_dataframe.withColumn(\"minute\", minute(\"timestamp\"))\n",
    "csld_spark_hh_dataframe = csld_spark_hh_dataframe.withColumn(\"second\", second(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 141:==================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+\n",
      "|timestamp          |value|home_id|roomtype|room_id|sensor_id|sensorboxtype|sensortype|sensor_status|day|month|year|hour|minute|second|\n",
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+\n",
      "|2018-03-07 19:37:55|1264 |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |7  |3    |2018|19  |37    |55    |\n",
      "|2018-03-07 22:56:20|2527 |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |7  |3    |2018|22  |56    |20    |\n",
      "|2018-03-08 07:25:11|316  |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |8  |3    |2018|7   |25    |11    |\n",
      "|2018-03-14 20:41:50|316  |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |14 |3    |2018|20  |41    |50    |\n",
      "|2018-03-15 23:51:01|316  |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |15 |3    |2018|23  |51    |1     |\n",
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# extract day, month, year, hour, minute, seconds from timestamp column\n",
    "csld_spark_hh_dataframe.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send the transformed data to PostgreSql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set up the connection\n",
    "pyspark_postgreSQL = \"jdbc:postgresql://localhost:5432/test_spark_db\"\n",
    "connx = {\n",
    "    \"user\": \"postgres\", \n",
    "    \"password\": \"4321\", \n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# define the dataframe to write to the database\n",
    "table_name = \"csld_spark_hh_dataframe\"\n",
    "\n",
    "# using jdbc connection to write data. mode can be overwrite, append, ignore, merge, ErrorIfExists\n",
    "csld_spark_hh_dataframe.write.format(\"jdbc\").option(\"url\",pyspark_postgreSQL)\\\n",
    "                            .option(\"dbtable\",table_name).options(**connx).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from the PostgreSql database to verify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 148:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+\n",
      "|          timestamp|value|home_id|roomtype|room_id|sensor_id|sensorboxtype|sensortype|sensor_status|day|month|year|hour|minute|second|\n",
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+\n",
      "|2018-03-08 19:21:18|  632|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0|  8|    3|2018|  19|    21|    18|\n",
      "|2018-03-10 20:54:56|  316|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0| 10|    3|2018|  20|    54|    56|\n",
      "|2018-03-12 17:24:22|  316|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0| 12|    3|2018|  17|    24|    22|\n",
      "|2018-03-16 00:38:03|  316|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0| 16|    3|2018|   0|    38|     3|\n",
      "|2018-03-16 17:27:21|  316|    308|     2.0|   2829|    19926|          2.0|       5.0|          0.0| 16|    3|2018|  17|    27|    21|\n",
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pyspark_postgreSQL = \"jdbc:postgresql://localhost:5432/test_spark_db\"\n",
    "connx = {\n",
    "    \"user\": \"postgres\", \n",
    "    \"password\": \"4321\", \n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "table_name = \"csld_spark_hh_dataframe\"\n",
    "\n",
    "new_data = spark.read.jdbc(url=pyspark_postgreSQL, table=table_name, properties=connx)\n",
    "\n",
    "new_data.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform with VectorAssembler and prepare data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 150:==============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+---------------------------------------------------------------------------+\n",
      "|timestamp          |value|home_id|roomtype|room_id|sensor_id|sensorboxtype|sensortype|sensor_status|day|month|year|hour|minute|second|features_value                                                             |\n",
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+---------------------------------------------------------------------------+\n",
      "|2018-03-07 19:37:55|1264 |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |7  |3    |2018|19  |37    |55    |[7.0,3.0,2018.0,19.0,37.0,55.0,1264.0,308.0,2.0,2829.0,19926.0,2.0,5.0,0.0]|\n",
      "|2018-03-07 22:56:20|2527 |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |7  |3    |2018|22  |56    |20    |[7.0,3.0,2018.0,22.0,56.0,20.0,2527.0,308.0,2.0,2829.0,19926.0,2.0,5.0,0.0]|\n",
      "|2018-03-08 07:25:11|316  |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |8  |3    |2018|7   |25    |11    |[8.0,3.0,2018.0,7.0,25.0,11.0,316.0,308.0,2.0,2829.0,19926.0,2.0,5.0,0.0]  |\n",
      "|2018-03-14 20:41:50|316  |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |14 |3    |2018|20  |41    |50    |[14.0,3.0,2018.0,20.0,41.0,50.0,316.0,308.0,2.0,2829.0,19926.0,2.0,5.0,0.0]|\n",
      "|2018-03-15 23:51:01|316  |308    |2.0     |2829   |19926    |2.0          |5.0       |0.0          |15 |3    |2018|23  |51    |1     |[15.0,3.0,2018.0,23.0,51.0,1.0,316.0,308.0,2.0,2829.0,19926.0,2.0,5.0,0.0] |\n",
      "+-------------------+-----+-------+--------+-------+---------+-------------+----------+-------------+---+-----+----+----+------+------+---------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# VectorAssembler\n",
    "# Select features to transform with VectorAssembler, please features in order of preference\n",
    "feature_for_ML = [\"day\",\"month\",\"year\",\"hour\",\"minute\",\"second\",\"value\",\"home_id\",\"roomtype\",\\\n",
    "                  \"room_id\",\"sensor_id\",\"sensorboxtype\",\"sensortype\",\"sensor_status\"]\n",
    "\n",
    "# the new column features_value contains the values of each feature in feature_for_ML\n",
    "csld_spark_hh_assembler = VectorAssembler(inputCols=feature_for_ML, outputCol=\"features_value\")\n",
    "\n",
    "# transformed consolidated_spark_hh_dataframe\n",
    "csld_spark_hh_for_ML = csld_spark_hh_assembler.transform(csld_spark_hh_dataframe)\n",
    "\n",
    "# peek into the output\n",
    "csld_spark_hh_for_ML.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         timestamp      value    home_id   roomtype  \\\n",
      "count                      1790951 1790951.00 1790951.00 1790951.00   \n",
      "unique                         NaN        NaN        NaN        NaN   \n",
      "top                            NaN        NaN        NaN        NaN   \n",
      "freq                           NaN        NaN        NaN        NaN   \n",
      "mean    2018-03-16 13:10:10.050400     341.26     308.00       0.23   \n",
      "min            2018-03-07 13:21:30       0.00     308.00       0.00   \n",
      "25%            2018-03-12 09:07:09     133.00     308.00       0.00   \n",
      "50%            2018-03-16 13:45:56     184.00     308.00       0.00   \n",
      "75%     2018-03-20 20:01:22.500000     301.00     308.00       0.00   \n",
      "max            2018-03-25 14:44:32   94781.00     308.00       2.00   \n",
      "std                            NaN     574.13       0.00       0.42   \n",
      "\n",
      "          room_id  sensor_id  sensorboxtype  sensortype  sensor_status  \\\n",
      "count  1790951.00 1790951.00     1790951.00  1790951.00     1790951.00   \n",
      "unique        NaN        NaN            NaN         NaN            NaN   \n",
      "top           NaN        NaN            NaN         NaN            NaN   \n",
      "freq          NaN        NaN            NaN         NaN            NaN   \n",
      "mean      2828.68   19931.37           0.23        0.56           0.00   \n",
      "min       2828.00   19920.00           0.00        0.00           0.00   \n",
      "25%       2828.00   19920.00           0.00        0.00           0.00   \n",
      "50%       2828.00   19920.00           0.00        0.00           0.00   \n",
      "75%       2828.00   19920.00           0.00        0.00           0.00   \n",
      "max       2831.00   19973.00           2.00        5.00           0.00   \n",
      "std          1.26      21.01           0.42        1.17           0.00   \n",
      "\n",
      "              day      month       year       hour     minute     second  \\\n",
      "count  1790951.00 1790951.00 1790951.00 1790951.00 1790951.00 1790951.00   \n",
      "unique        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "top           NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "freq          NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "mean        16.03       3.00    2018.00      11.93      29.39      29.48   \n",
      "min          7.00       3.00    2018.00       0.00       0.00       0.00   \n",
      "25%         12.00       3.00    2018.00       6.00      14.00      14.00   \n",
      "50%         16.00       3.00    2018.00      12.00      29.00      29.00   \n",
      "75%         20.00       3.00    2018.00      18.00      44.00      45.00   \n",
      "max         25.00       3.00    2018.00      23.00      59.00      59.00   \n",
      "std          5.18       0.00       0.00       6.94      17.31      17.32   \n",
      "\n",
      "                                           features_value  \n",
      "count                                             1790951  \n",
      "unique                                            1790951  \n",
      "top     [7.0, 3.0, 2018.0, 19.0, 37.0, 55.0, 1264.0, 3...  \n",
      "freq                                                    1  \n",
      "mean                                                  NaN  \n",
      "min                                                   NaN  \n",
      "25%                                                   NaN  \n",
      "50%                                                   NaN  \n",
      "75%                                                   NaN  \n",
      "max                                                   NaN  \n",
      "std                                                   NaN  \n"
     ]
    }
   ],
   "source": [
    "# some final statistics of the dataset, using the stats_csld_spark_hh_for_ML\n",
    "\n",
    "with pd.option_context('display.float_format', '{:.2f}'.format):\n",
    "    stats_csld_spark_hh_for_ML = csld_spark_hh_for_ML.toPandas().describe(include='all')\n",
    "    print(stats_csld_spark_hh_for_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminate the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
